import &path, &ArtStandardLib, &ArtClassSystem, &Lib
colors = terminalColors

S3C = &S3Comprehensions

itemsByKey = (itemList, toKey) ->
  object {Key, Size} in-array itemList with-key
      if toKey?
        toKey Key
      else Key
    Size

summarizeHistogramGroups =
  :bytes
  :kilobytes
  :megabytes
  :gigabytes
  :terabytes
  :petabytes
  :exabytes
  :zetabytes

summarizeHistogramGroupUnits =
  :_B
  :kB
  :mB
  :gB
  :tB
  :pB
  :eB
  :zB

class S3P
  ###########################################################
    READONLY Commands
  ###########################################################
  @list: (options) =>
    S3C.each merge options,
      returning: list = []
      mapList: (l) -> array from-array l into list

  @summarize: (options) =>
    options extract summarizeFolders
    summary =
      size: 0
      maxSize: null
      minSize: null
      maxSizeKey: null
      minSizeKey: null
      sizeHistogram: {}
      folders: summarizeFolders && {}

    S3C.each merge options,
      getProgress: ->
        []
          "" totalSize: #{} colors.green "" #{humanByteSize summary.size}
          "" minSize: #{} colors.green "" #{humanByteSize summary.minSize}
          "" maxSize: #{} colors.green "" #{humanByteSize summary.maxSize}
        .join ' '

      map: ({Size, Key}) ->
        floorSize = humanByteSize
          Math.pow 2, 1 + logSize = (Math.log(Size) / Math.log(2)) | 0
          0

        group = summarizeHistogramGroups[groupKey = ((logSize + 1)/10) | 0] ? :big
        groupUnit = summarizeHistogramGroupUnits[groupKey]
        g = summary.sizeHistogram[group] ?=
          each i til 10 into out = items: 0 size: 0
            out[(1 << i) + groupUnit] = 0

        g.items += 1
        g.size += Size
        g[floorSize] = (g[floorSize] | 0) + 1

        if summarizeFolders
          folder = summary.folders
          each subFolder in-array
              dirname Key
              .split '/'

            folder = folder[subFolder] ?= size: 0, files: 0
            folder.size += Size
            folder.files++

        summary.size += Size
        if Size >= summary.maxSize ? Size
          summary.maxSize = Size
          summary.maxSizeKey = Key

        if Size <= summary.minSize ? Size
          summary.minSize = Size
          summary.minSizeKey = Key

        summary.minSize = Math.min
          summary.minSize ? Size
          Size

    .then (stats) ->
      summary.averageSize = summary.size / (stats.matchingItems ? stats.items) | 0
      if summarizeFolders
        humanize = (folder) ->
          folder.humanSize = humanByteSize folder.size if folder.size is Number
          each subFolder in-object folder when subFolder is Object
            humanize subFolder

        humanize summary.folders

      merge
        stats
        summary
        {}
          human: object v, k in-object summary with humanByteSize v when /size$/i.test k

  @compare: (options) =>
    options = S3C.normalizeOptions options
    options extract
      logToDelete, logToCopy, logToReplace, verbose, toKey
      bucket
      fromFolder
      toBucket
      toFolder
    unless toBucket
      throw new Error "toBucket required"

    S3C.each merge
      options,
      returning: stats = {}
        counts =
          needToCopy:         0
          needToReplace:      0
          needToDelete:       0
          missingInTarget:    0
          different:          0
          replaceSmaller:     0
          replaceBigger:      0
          same:               0
        bytes =
          needToCopy:         0
          needToDelete:       0
          needToReplace:      0
          needToReplaceWith:  0
          same:               0

      getProgress: ->
        compactFlatten []
          if counts.same > 0          then "" same: #{} colors.green "" #{counts.same}(#{humanByteSize bytes.same})
          if counts.needToCopy > 0    then "" toCopy: #{} colors.green "" #{counts.needToCopy}(#{humanByteSize bytes.needToCopy})
          if counts.needToReplace > 0 then "" toReplace: #{} colors.green "" #{counts.needToReplace}(#{humanByteSize bytes.needToReplace} with #{humanByteSize bytes.needToReplaceWith})
          if counts.needToDelete > 0  then "" toDelete: #{} colors.green "" #{counts.needToDelete}(#{humanByteSize bytes.needToDelete})
        .join ' '

      compare: true
      mapList: (sourceItems, targetItems) ->

        reverseKeyMap = toKey &&  object {Key} in sourceItems with-key toKey Key with Key

        objectDiff
          itemsByKey sourceItems, toKey  # , options.prefix # right now these are somewhat asymetric - the source prefix is mainatined when copying to a prefix
          itemsByKey targetItems
          # added
          (key, sourceValue) ->
            fromKey = reverseKeyMap?[key] ? key
            log "aws s3 cp #{createS3Url bucket, fromFolder, fromKey} #{createS3Url toBucket, toFolder, key} # #{humanByteSize sourceValue}" if logToCopy || verbose
            bytes.needToCopy += sourceValue; counts.needToCopy++; counts.missingInTarget++

          # removed
          (key, targetValue) ->
            log "rm #{createS3Url toBucket, toFolder, key} # #{humanByteSize targetValue}" if logToDelete || verbose
            bytes.needToDelete += targetValue; counts.needToDelete++

          # changed
          (key, sourceValue, targetValue) ->
            fromKey = reverseKeyMap?[key] ? key
            log "aws s3 cp #{createS3Url bucket, fromFolder, fromKey} #{createS3Url toBucket, toFolder, key} # replace #{humanByteSize targetValue} with #{humanByteSize sourceValue}" if logToReplace || verbose
            counts.different++
            counts.needToReplace++
            bytes.needToReplace+=targetValue
            bytes.needToReplaceWith+=sourceValue
            if sourceValue > targetValue
                              counts.replaceSmaller++
            else              counts.replaceBigger++

          # same
          (key, value) ->
            bytes.same += value
            counts.same++

          # compare
          # (item1, item2) -> item1.Size == item2.Size

    .then (stats) ->
      cleanStats = (s) ->
        object v in s when v != 0
          if v is Object
            cleanStats v
          else v
      cleanStats stats

  ###########################################################
    READ/WRITE Commands
  ###########################################################
  ##
    IN: options:
      all the each options plus:
      toBucket: <String> [Default: options.bucket] - target bucket
      toPrefix: <String> optional - prepend to key for toKey
      toKey: (optional) <Function> (fromKey, fromBucket, toBucket, size) -> to-key-string
      copyConcurrency       [100]                   number of active copies < 1GB using aws sdk directly
      largeCopyConcurrency  [copyConcurrency/4]     number of active copies > 1GB using exec aws cp
      maxQueueSize          [copyConcurrency * 10]  max number of copy-jobs to queue before throttling s3.lists
  @copy: (options) =>
    @_copyWrapper options, (updatedOptions) =>
      S3C.eachPromises updatedOptions

  @sync: (options) =>
    @_copyWrapper options, (options2) ->
      options2 extract
        map as copyFile
        stats
        overwrite
        dryrun
        pretend = dryrun
        toKey
        toBucket

      stats.toDeleteFiles = 0
      stats.toDeleteBytes = 0
      stats.toReplaceFiles = 0
      stats.toReplaceBytes = 0
      stats.toReplaceWithBytes = 0
      stats.replacedFiles = 0
      stats.replacedBytes = 0

      stats.unchangedFiles = 0
      stats.unchangedBytes = 0

      S3C.each merge
        objectWithout options2, :map
        compare: true
        getProgress: (duration) ->
          options2.getProgress duration
          + ""
            \_same: #{} colors.green "" #{stats.unchangedFiles}(#{humanByteSize stats.unchangedBytes})
            toDelete: #{} colors.green "" #{stats.toDeleteFiles}(#{humanByteSize stats.toDeleteBytes})

        mapList: (sourceItems, targetItems) ->
          copyPromises = []

          reverseKeyMap = toKey && object {Key} in sourceItems with-key toKey Key with Key

          objectDiff
            itemsByKey sourceItems, toKey
            itemsByKey targetItems
            # added
            (key, sourceValue) ->
              fromKey = reverseKeyMap?[key] ? key
              copyPromises.push copyFile {} Key: fromKey, Size: sourceValue

            # removed
            (key, targetValue) ->
              log "aws s3 rm s3://#{toBucket}/#{key} # you must do this. size: #{humanByteSize targetValue}"
              stats.toDeleteBytes += targetValue
              stats.toDeleteFiles++

            # changed
            (key, sourceValue, targetValue) ->
              fromKey = reverseKeyMap?[key] ? key
              if overwrite
                log "# overwriting s3://#{toBucket}/#{key} - replacing targetSize: #{targetValue} with sourceSize #{sourceValue}"
                copyPromises.push copyFile {} Key: fromKey, Size: sourceValue
                stats.replacedFiles++
                stats.replacedBytes += targetValue
              else
                log "# NOT overwriting s3://#{toBucket}/#{key} - replacing targetSize: #{targetValue} with sourceSize #{sourceValue} (use overwrite: true to overwrite)"
                stats.toReplaceFiles++
                stats.toReplaceBytes += targetValue
                stats.toReplaceWithBytes = sourceValue

            # same
            (key, value) ->
              stats.unchangedBytes += value
              stats.unchangedFiles++

          Promise.all copyPromises

  ######################################################
    PRIVATE
  ######################################################

  ## @_copyWrapper is used by @copy and @sync to share copy-code
    IN:
      options - all the options you pass to @copy or @sync
      eachFunction: (updatedOptions) ->

      updatedOptions are options which can be passed directly to S3C.each.
      In particular, @_copyWrapper defines crical shared copy-code:
        map: ->
        getProgress: ->
        throttle: ->

      It also creates the basic stats object, which it uses to return the final results.
      TODO: this stats-object with final 'humanize' should become the base line for all S3P

  @_copyWrapper: (options, eachFunction) =>
    options = S3C.normalizeOptions options
    options extract
      s3
      toKey
      stats
      pretend
      verbose
      copyConcurrency
      largeCopyConcurrency
      maxQueueSize
      copyPwp
      largeCopyPwp
      largeCopyThreshold

      # s3 options
      region
      useAccelerateEndpoint
    s3 ?= new &Lib/S3 {} region, useAccelerateEndpoint

    stats ?= {}
    stats.copiedBytes = 0
    stats.copiedBytesPerSecond = 0
    stats.copyingBytesInFlight = 0
    stats.copyingBytesStarted = 0
    stats.copyingFilesStarted = 0
    stats.copiedFiles = 0

    throw new Error "toBucket or toFolder required" unless (present options.toBucket) || present options.toFolder
    eachFunction merge options, {}
      stats
      throttle: -> copyPwp.queueSize + largeCopyPwp.queueSize >= maxQueueSize
      getProgress: (duration) ->
        compactFlatten []
          "" copied
          "" #{} colors.green "" #{stats.copiedFiles}/#{stats.copyingFilesStarted} #{humanByteSize stats.copiedBytes}/#{humanByteSize stats.copyingBytesStarted}
          colors.blue "" #{humanByteSize stats.copiedBytesPerSecond = stats.copiedBytes / duration}/s
          "" inFlight: #{} colors.green humanByteSize stats.copyingBytesInFlight
          "" copyWorkers: #{} colors.green "" #{copyPwp.activeWorkers} + #{largeCopyPwp.activeWorkers}
          if 0 < copyPwp.queueSize + largeCopyPwp.queueSize
            "" copyQueue: #{} colors.green "" #{copyPwp.queueSize} + #{largeCopyPwp.queueSize}
          colors.yellow "PRETENDING" if pretend
        .join ' '

      map: ({Key: key, Size}) ->
        stats.copyingFilesStarted++
        stats.copyingBytesStarted += Size

        if Size < largeCopyThreshold then copyPwp else largeCopyPwp
        .queue ->
          stats.copyingBytesInFlight += Size
          options = merge
            s3.extractCommonCopyOptions options
            {}
              pretend
              verbose
              options.bucket
              options.toBucket
              options.toFolder
              key
              toKey
              largeCopyThreshold
              size: Size

          s3.copy options
          .then ->
            stats.copyingBytesInFlight -= Size
            stats.copiedFiles++
            stats.copiedBytes += Size

          .catch (error) ->
            if /NoSuchKey: The specified key does not exist/.test error.stack
              if verbose then log "" skipping because "NoSuchKey: The specified key does not exist": #{key} (#{Size} bytes)
              stats.keyDoesNotExistFiles =
                stats.keyDoesNotExistFiles ? 0
                + 1
              stats.keyDoesNotExistBytes =
                stats.keyDoesNotExistBytes ? 0
                + Size
            else throw error

    .then (stats) ->
      delete stats.copyingBytesInFlight
      delete stats.copyingFilesStarted
      delete stats.copyingBytesStarted
      stats.copiedBytesPerSecond = stats.copiedBytes / stats.duration
      finalStats: @getStatsWithHumanByteSizes stats

  @getStatsWithHumanByteSizes: (stats) ->
    merge
      object v in stats
        if v is Number
          if abs(v) < 100
            round v, .01
          else
            round v

        else v
      human: object stat, key in stats when /byte|size/i.test key
        humanByteSize stat
