import &StandardImport, &FsEasy, &LibMisc, &@aws-sdk/client-s3, {} &shellEscape, &util.promisify, &fs, &path
escape = (single) -> shellEscape [] single

# &@aws-sdk/client-s3.config.setPromisesDependency &bluebird

class S3 extends BaseClass

  @awsSdkS3 = new S3Client {} # useAccelerateEndpoint: true
  # https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html

  @classGetter
    s3: ->
      listBuckets:    (options = {}) -> Promise.resolve @awsSdkS3.send new ListBucketsCommand options
      listObjectsV2:  (options = {}) -> Promise.resolve @awsSdkS3.send new ListObjectsV2Command options
      getObject:      (options = {}) -> Promise.resolve @awsSdkS3.send new GetObjectCommand options
      copyObject:     (options = {}) -> Promise.resolve @awsSdkS3.send new CopyObjectCommand options
      deleteObject:   (options = {}) -> Promise.resolve @awsSdkS3.send new DeleteObjectCommand options
      headObject:     (options = {}) -> Promise.resolve @awsSdkS3.send new HeadObjectCommand options

  @commonCopyOptionsSdkKeys =
    :ACL
    :CacheControl
    :ContentDisposition
    :ContentEncoding
    :ContentLanguage
    :ContentType
    :Expires
    :RequestPayer
    :StorageClass

  @commonSdkCopyOptionsByLowerCamelCaseKeys = object k from @commonCopyOptionsSdkKeys with-key lowerCamelCase k

  @listBuckets: =>
    @s3.listBuckets()
    .then ({Buckets}) ->
      object {Name, CreationDate} from Buckets with-key Name with CreationDate

  @list: ({bucket, prefix, limit=1000, fetchOwner, startAfter}) =>
    startTime = currentSecond()
    @s3.listObjectsV2
      Bucket:     bucket
      Prefix:     prefix
      MaxKeys:    limit
      StartAfter: startAfter
      FetchOwner: fetchOwner
    .tapCatch (error) ->
      log.error S3.list-error: {} bucket, prefix, startAfter, limit, error

    .then (results) ->
      duration = currentSecond() - startTime
      if !results.Contents? then results.Contents = []
      unless results.Contents is Array
        log.warn S3.list-no-Contents: {} bucket, prefix, startAfter, limit, duration, results
        throw new Error "S3.list: Contents is not an array"

      else if duration > 60
        log.warn S3.list-slow: {} bucket, prefix, startAfter, limit, duration, results: merge results, Contents: "Array #{results.Contents.length}"

      results.Contents

  ##
    TODO:
      For large files (>5GB), we need to just use the CLI:
        aws s3 cp s3://resbio-fastq-drop-internal/180316_NB501104_0354_AHM2L2BGX5/Undetermined/Undetermined_S0_L001_R1_001.fastq.gz s3://genui-dynamodb-migration-scratch/large-test.gz
        We can add --quiet (probably)
      Further, that should use acceleration, if enabled - still need to test that.
    IN:
      options:
        size:       Bytes (optional) if provided and if bigger than the max size allowed by copyObject, largeCopy is used
        bucket:     <String> (ignored if both from-bucket and to-bucket are provided)
        fromBucket: from-bucket (default: bucket)
        toBucket:   to-bucket (default: bucket)
        key:      <String>
        fromKey:  from-key (default: key)
        toKey:    to-key (default: key)

        toFolder:   local folder
    OUT:
      Promise.then -> see https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#copyObject-property > Callback > Parameters
        {} CopyObjectResult, ...
  @copy: (options) =>
    if options extract largeCopyThreshold, scratchState
      copyScratchState = scratchState.copyScratchState ?= {}

    @_normalizeCopyOptions(options) extract
      fromBucket, toBucket, fromKey, toKey, size
      toFolder
      pretend
      verbose

    if size >= largeCopyThreshold
      @largeCopy {}
        commonCopyOptions: @extractCommonCopyOptions options
        fromBucket, toBucket, toFolder, fromKey, toKey, pretend, verbose

    else
      copyOptions = merge
        if toFolder
          Bucket:     fromBucket
          Key:        fromKey

        else
          CopySource: encodeURIComponent "" #{fromBucket}/#{fromKey}
          Bucket:     toBucket
          Key:        toKey

        @_getCommonCopyOptionsForSdk options

      toLocalFile = if toFolder then path.join toFolder, toKey

      if verbose
        log.unquoted merge {}
          copyObject: copyOptions
          toLocalFile

      if pretend
        timeout 1 -> pretend: true

      else if toFolder
        createWriteStreamSafe
          toLocalFile
          copyScratchState

        .then (writeStream) -> new Promise (resolve, reject) ->
          @s3.getObject copyOptions
          .then ({Body}) ->
            Body
            .pipe(writeStream);
            .on :finish resolve
            .on :error  reject

      else
        @s3.copyObject copyOptions

  @delete: (options) =>
    @s3.deleteObject
      Bucket: options.bucket
      Key:    options.key

  @largeCopy: (options) =>
    options extract commonCopyOptions
    @_normalizeCopyOptions(options) extract
      fromBucket,
      fromFolder, toBucket,
      toFolder, fromKey,
      toKey, pretend, verbose

    command =
      compactFlatten []
        ""
          aws s3 cp
          #{} escape createS3Url fromBucket, fromFolder, fromKey
          #{} escape createS3Url toBucket, toFolder, toKey
        array v, key from commonCopyOptions when v
          "" --#{dashCase key} #{}
            switch v
            when true then ''
            else escape v

      .join ' '

    if verbose
      log.unquoted largeCopy: command

    if pretend
      timeout 1 -> pretend: true
    else
      shellExec command

  # get object's metdata
  @headObject: ({bucket, key}) =>
    @s3.headObject
      Bucket: bucket
      Key: key

  @_normalizeCopyOptions: (options) ->
    options extract
      bucket, key
      fromBucket = bucket
      toBucket = bucket
      fromKey = key
      toKey = key
      size

    if isFunction toKey
      toKey = toKey fromKey, fromBucket, toBucket, options.size

    unless present(fromBucket) && present(toBucket) && present(fromKey) && present toKey
      throw new Error "" Missing one of: fromBucket, toBucket, fromKey, toKey or bucket or key as a default
    merge options, {} fromBucket, fromKey, toBucket, toKey

  @_getCommonCopyOptionsForSdk: (options) =>
    object sdkKey, lowerCamelCasedKey from @commonSdkCopyOptionsByLowerCamelCaseKeys with-key sdkKey
      options[lowerCamelCasedKey]

  @extractCommonCopyOptions: (options) =>
    object v, k from @commonSdkCopyOptionsByLowerCamelCaseKeys
      options[k]

  @shouldSyncObjects: (options) ->
    @_normalizeCopyOptions(options) extract fromBucket, toBucket, fromKey, toKey
    Promise.then ->
      if options.size < 1024**2 || !options.size
        true
      else
        @headObject options
        .then ({ContentLength}) -> ContentLength != options.size

  ## syncObject
    IN: options:
      passed to copyObject and shouldSyncObject
      size: used to compare and decide if it should copy
    OUT:
      copied: Bool
      status: if not copied, explains why
  @syncObject: (options) =>
    @shouldSyncObjects options
    .then (shouldSync) ->
      if shouldSync
        @copyObject options
        .then (result) -> merge result, copied: true

      else {} copied: false
        # status: "s3://#{toBucket}/#{toKey} exists and has same size (#{size}); skipping"
