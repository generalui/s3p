import &StandardImport, &FsEasy, &LibMisc, &@aws-sdk/client-s3, &@aws-sdk/lib-storage, {} &shellEscape, &util.promisify, &path
escape = (single) -> shellEscape [] single

fs = merge
  {createReadStream} = &fs
  {copyFile} = &fs.promises

# &@aws-sdk/client-s3.config.setPromisesDependency &bluebird

class S3 extends BaseClass

  @commonCopyOptionsSdkKeys =
    :ACL
    :CacheControl
    :ContentDisposition
    :ContentEncoding
    :ContentLanguage
    :ContentType
    :Expires
    :RequestPayer
    :StorageClass

  @commonSdkCopyOptionsByLowerCamelCaseKeys = object k from @commonCopyOptionsSdkKeys with-key lowerCamelCase k

  ## constructor
    IN: options:
      region: string, default: process.env.AWS_REGION
      useAccelerateEndpoint: boolean, default: false

  constructor: (options = []) ->
    @awsSdkS3 = new S3Client merge
      region: process.env.AWS_REGION
      {region, useAccelerateEndpoint} = options

  @getter
    s3: ->
      listBuckets:    (options = {}) -> Promise.resolve @awsSdkS3.send new ListBucketsCommand options
      listObjectsV2:  (options = {}) -> Promise.resolve @awsSdkS3.send new ListObjectsV2Command options
      getObject:      (options = {}) -> Promise.resolve @awsSdkS3.send new GetObjectCommand options
      copyObject:     (options = {}) -> Promise.resolve @awsSdkS3.send new CopyObjectCommand options
      deleteObject:   (options = {}) -> Promise.resolve @awsSdkS3.send new DeleteObjectCommand options
      headObject:     (options = {}) -> Promise.resolve @awsSdkS3.send new HeadObjectCommand options


  listBuckets: ->
    @s3.listBuckets()
    .then ({Buckets}) ->
      object {Name, CreationDate} from Buckets with-key Name with CreationDate

  ## list
    OUT:
      []
        Key:
        LastModified:
        Size
        ETag
        StorageClass
        Owner
  list: ({bucket, prefix, limit=1000, fetchOwner, startAfter, stopAt}) =>
    startTime = currentSecond()
    @s3.listObjectsV2
      Bucket:     bucket
      Prefix:     prefix
      MaxKeys:    limit
      StartAfter: startAfter
      FetchOwner: fetchOwner

    .tapCatch (error) ->
      log.error S3.list-error: {} bucket, prefix, startAfter, limit, error

    .then (results) ->
      duration = currentSecond() - startTime
      if !results.Contents? then results.Contents = []
      unless results.Contents is Array
        log.warn S3.list-no-Contents: {} bucket, prefix, startAfter, limit, duration, results
        throw new Error "S3.list: Contents is not an array"

      else if duration > 60
        log.warn S3.list-slow: {} bucket, prefix, startAfter, limit, duration, results: merge results, Contents: "Array #{results.Contents.length}"

      if present stopAt
        array item in-array results.Contents when item.Key <= stopAt
      else
        results.Contents

  ## copy
    IN:
      options:
        size:       Bytes (optional) if provided and if bigger than the max size allowed by copyObject, largeCopy is used
        bucket:     <String> (ignored if both from-bucket and to-bucket are provided)
        fromBucket: from-bucket (default: bucket)
        toBucket:   to-bucket (default: bucket)
        key:      <String>
        fromKey:  from-key (default: key)
        toKey:    to-key (default: key)
        fromFolder: string

        toFolder:   local folder
    OUT:
      Promise.then -> see https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#copyObject-property > Callback > Parameters
        {} CopyObjectResult, ...
  copy: (options) ->
    if options extract largeCopyThreshold, scratchState
      copyScratchState = scratchState.copyScratchState ?= {}

    @_normalizeCopyOptions(options) extract
      fromBucket, fromFolder, fromKey
      toBucket, toFolder, toKey
      pretend, verbose
      size

    if size >= largeCopyThreshold
      @largeCopy {}
        commonCopyOptions: @extractCommonCopyOptions options
        fromBucket, fromFolder, fromKey
        toBucket, toFolder, toKey
        pretend, verbose

    else
      copyOptions = merge
        if toFolder
          Bucket:     fromBucket
          Key:        fromKey

        else
          CopySource: encodeURIComponent "" #{fromBucket}/#{fromKey}
          Bucket:     toBucket
          Key:        toKey

        @_getCommonCopyOptionsForSdk options

      toLocalFile = if toFolder then path.join toFolder, toKey

      if verbose
        log.unquoted merge {}
          copyObject: copyOptions
          toLocalFile

      if pretend
        timeout 1 -> pretend: true

      else if fromFolder
        fromFilePath = path.join fromFolder, fromKey
        if toBucket
          new Upload
            client: @awsSdkS3
            params:
              Bucket: toBucket
              Key:    toKey
              Body:   fs.createReadStream fromFilePath

          .done()

        else
          createParentDirs toLocalFile, copyScratchState
          .then -> fs.copyFile fromFilePath, toLocalFile

      else if toFolder
        if /\/$/.test(toKey) && size == 0 # this is a folder
          createParentDirs
            path.join toLocalFile, "ignored-file-name"
            copyScratchState

        else
          createWriteStreamSafe toLocalFile, copyScratchState
          .then (writeStream) -> new Promise (resolve, reject) ->
            @s3.getObject copyOptions
            .then ({Body}) ->
              Body
              .pipe(writeStream);
              .on :finish resolve
              .on :error  reject

      else
        @s3.copyObject copyOptions

  delete: (options) ->
    @s3.deleteObject
      Bucket: options.bucket
      Key:    options.key

  largeCopy: (options) ->
    options extract commonCopyOptions
    @_normalizeCopyOptions(options) extract
      fromBucket,
      fromFolder, toBucket,
      toFolder, fromKey,
      toKey, pretend, verbose

    command =
      compactFlatten []
        ""
          aws s3 cp
          #{} escape createS3Url fromBucket, fromFolder, fromKey
          #{} escape createS3Url toBucket, toFolder, toKey
        array v, key from commonCopyOptions when v
          "" --#{dashCase key} #{}
            switch v
            when true then ''
            else escape v

      .join ' '

    if verbose
      log.unquoted largeCopy: command

    if pretend
      timeout 1 -> pretend: true
    else
      shellExec command

  # get object's metadata
  @headObject: ({bucket, key}) =>
    @s3.headObject
      Bucket: bucket
      Key: key

  _normalizeCopyOptions: (options) ->
    options extract
      bucket, key
      fromBucket = bucket
      toBucket = bucket
      fromKey = key
      toKey = key
      size

    if isFunction toKey
      toKey = toKey fromKey, fromBucket, toBucket, options.size

    unless present(fromBucket) && present(toBucket) && present(fromKey) && present toKey
      throw new Error "" Missing one of: fromBucket, toBucket, fromKey, toKey or bucket or key as a default
    merge options, {} fromBucket, fromKey, toBucket, toKey

  _getCommonCopyOptionsForSdk: (options) ->
    object sdkKey, lowerCamelCasedKey from S3.commonSdkCopyOptionsByLowerCamelCaseKeys with-key sdkKey
      options[lowerCamelCasedKey]

  extractCommonCopyOptions: (options) ->
    object v, k from S3.commonSdkCopyOptionsByLowerCamelCaseKeys
      options[k]

  shouldSyncObjects: (options) ->
    @_normalizeCopyOptions(options) extract fromBucket, toBucket, fromKey, toKey
    Promise.then ->
      if options.size < 1024**2 || !options.size
        true
      else
        @headObject options
        .then ({ContentLength}) -> ContentLength != options.size

  ## syncObject
    IN: options:
      passed to copyObject and shouldSyncObject
      size: used to compare and decide if it should copy
    OUT:
      copied: Bool
      status: if not copied, explains why
  syncObject: (options) ->
    @shouldSyncObjects options
    .then (shouldSync) ->
      if shouldSync
        @copyObject options
        .then (result) -> merge result, copied: true

      else {} copied: false
        # status: "s3://#{toBucket}/#{toKey} exists and has same size (#{size}); skipping"
